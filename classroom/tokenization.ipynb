{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\"Tokens\" are usually individual words (at least in languages like English) and \"tokenization\" is taking a text or set of text and breaking it up into its individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Using the power of the Infinity Stones, Thanos believes he can ultimately save the universe by wiping out half of its population.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words =  word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'the', 'power', 'of', 'the', 'Infinity', 'Stones', ',', 'Thanos', 'believes', 'he', 'can', 'ultimately', 'save', 'the', 'universe', 'by', 'wiping', 'out', 'half', 'of', 'its', 'population', '.']\n"
     ]
    }
   ],
   "source": [
    "print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "print (len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"what's up?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', \"'s\", 'up', '?']\n"
     ]
    }
   ],
   "source": [
    "print (word_tokenize(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', \"'\", 's', 'up', '?']\n"
     ]
    }
   ],
   "source": [
    "print (wordpunct_tokenize(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using the power of the Infinity Stones, Thanos believes he can ultimately save the universe by wiping out half of its population.']\n"
     ]
    }
   ],
   "source": [
    "print (sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print (len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It's about to rain.\", 'I need an umbrella.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(\"It's about to rain. I need an umbrella.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pair of consecutive written units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I thought by eliminating half of life, the other half would thrive. But you have shown me... that's impossible. As long as there are those who can remember what was, there will always be those that are unable to accept what can be. They will resist.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'thought'),\n",
       " ('thought', 'by'),\n",
       " ('by', 'eliminating'),\n",
       " ('eliminating', 'half'),\n",
       " ('half', 'of'),\n",
       " ('of', 'life'),\n",
       " ('life', ','),\n",
       " (',', 'the'),\n",
       " ('the', 'other'),\n",
       " ('other', 'half'),\n",
       " ('half', 'would'),\n",
       " ('would', 'thrive'),\n",
       " ('thrive', '.'),\n",
       " ('.', 'But'),\n",
       " ('But', 'you'),\n",
       " ('you', 'have'),\n",
       " ('have', 'shown'),\n",
       " ('shown', 'me'),\n",
       " ('me', '...'),\n",
       " ('...', 'that'),\n",
       " ('that', \"'s\"),\n",
       " (\"'s\", 'impossible'),\n",
       " ('impossible', '.'),\n",
       " ('.', 'As'),\n",
       " ('As', 'long'),\n",
       " ('long', 'as'),\n",
       " ('as', 'there'),\n",
       " ('there', 'are'),\n",
       " ('are', 'those'),\n",
       " ('those', 'who'),\n",
       " ('who', 'can'),\n",
       " ('can', 'remember'),\n",
       " ('remember', 'what'),\n",
       " ('what', 'was'),\n",
       " ('was', ','),\n",
       " (',', 'there'),\n",
       " ('there', 'will'),\n",
       " ('will', 'always'),\n",
       " ('always', 'be'),\n",
       " ('be', 'those'),\n",
       " ('those', 'that'),\n",
       " ('that', 'are'),\n",
       " ('are', 'unable'),\n",
       " ('unable', 'to'),\n",
       " ('to', 'accept'),\n",
       " ('accept', 'what'),\n",
       " ('what', 'can'),\n",
       " ('can', 'be'),\n",
       " ('be', '.'),\n",
       " ('.', 'They'),\n",
       " ('They', 'will'),\n",
       " ('will', 'resist'),\n",
       " ('resist', '.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
